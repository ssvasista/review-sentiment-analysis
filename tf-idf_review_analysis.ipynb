{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following python program is using TF-IDF text representations and performing text classication for sentiment analysis on Amazon Review data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/shravanvasista/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/shravanvasista/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/shravanvasista/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import bs4\n",
    "import contractions\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install bs4 # in case you don't have it installed\n",
    "\n",
    "# Dataset: https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Kitchen_v1_00.tsv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>37000337</td>\n",
       "      <td>R3DT59XH7HXR9K</td>\n",
       "      <td>B00303FI0G</td>\n",
       "      <td>529320574</td>\n",
       "      <td>Arthur Court Paper Towel Holder</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Beautiful. Looks great on counter</td>\n",
       "      <td>Beautiful.  Looks great on counter.</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>15272914</td>\n",
       "      <td>R1LFS11BNASSU8</td>\n",
       "      <td>B00JCZKZN6</td>\n",
       "      <td>274237558</td>\n",
       "      <td>Olde Thompson Bavaria Glass Salt and Pepper Mi...</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Awesome &amp; Self-ness</td>\n",
       "      <td>I personally have 5 days sets and have also bo...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>36137863</td>\n",
       "      <td>R296RT05AG0AF6</td>\n",
       "      <td>B00JLIKA5C</td>\n",
       "      <td>544675303</td>\n",
       "      <td>Progressive International PL8 Professional Man...</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Fabulous and worth every penny</td>\n",
       "      <td>Fabulous and worth every penny. Used for clean...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>43311049</td>\n",
       "      <td>R3V37XDZ7ZCI3L</td>\n",
       "      <td>B000GBNB8G</td>\n",
       "      <td>491599489</td>\n",
       "      <td>Zyliss Jumbo Garlic Press</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>A must if you love garlic on tomato marinara s...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>13763148</td>\n",
       "      <td>R14GU232NQFYX2</td>\n",
       "      <td>B00VJ5KX9S</td>\n",
       "      <td>353790155</td>\n",
       "      <td>1 X Premier Pizza Cutter - Stainless Steel 14\"...</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Better than sex</td>\n",
       "      <td>Worth every penny! Buy one now and be a pizza ...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  marketplace  customer_id       review_id  product_id  product_parent  \\\n",
       "0          US     37000337  R3DT59XH7HXR9K  B00303FI0G       529320574   \n",
       "1          US     15272914  R1LFS11BNASSU8  B00JCZKZN6       274237558   \n",
       "2          US     36137863  R296RT05AG0AF6  B00JLIKA5C       544675303   \n",
       "3          US     43311049  R3V37XDZ7ZCI3L  B000GBNB8G       491599489   \n",
       "4          US     13763148  R14GU232NQFYX2  B00VJ5KX9S       353790155   \n",
       "\n",
       "                                       product_title product_category  \\\n",
       "0                    Arthur Court Paper Towel Holder          Kitchen   \n",
       "1  Olde Thompson Bavaria Glass Salt and Pepper Mi...          Kitchen   \n",
       "2  Progressive International PL8 Professional Man...          Kitchen   \n",
       "3                          Zyliss Jumbo Garlic Press          Kitchen   \n",
       "4  1 X Premier Pizza Cutter - Stainless Steel 14\"...          Kitchen   \n",
       "\n",
       "   star_rating  helpful_votes  total_votes vine verified_purchase  \\\n",
       "0          5.0            0.0          0.0    N                 Y   \n",
       "1          5.0            0.0          1.0    N                 Y   \n",
       "2          5.0            0.0          0.0    N                 Y   \n",
       "3          5.0            0.0          1.0    N                 Y   \n",
       "4          5.0            0.0          0.0    N                 Y   \n",
       "\n",
       "                     review_headline  \\\n",
       "0  Beautiful. Looks great on counter   \n",
       "1                Awesome & Self-ness   \n",
       "2     Fabulous and worth every penny   \n",
       "3                         Five Stars   \n",
       "4                    Better than sex   \n",
       "\n",
       "                                         review_body review_date  \n",
       "0                Beautiful.  Looks great on counter.  2015-08-31  \n",
       "1  I personally have 5 days sets and have also bo...  2015-08-31  \n",
       "2  Fabulous and worth every penny. Used for clean...  2015-08-31  \n",
       "3  A must if you love garlic on tomato marinara s...  2015-08-31  \n",
       "4  Worth every penny! Buy one now and be a pizza ...  2015-08-31  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data = pd.read_csv('amazon_reviews_us_Kitchen_v1_00.tsv', sep='\\t', usecols=['star_rating','review_body'])\n",
    "df = pd.read_csv('https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Kitchen_v1_00.tsv.gz', compression='gzip', sep='\\t', warn_bad_lines=False, error_bad_lines=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep Reviews and Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   star_rating                                        review_body\n",
      "0          5.0                Beautiful.  Looks great on counter.\n",
      "1          5.0  I personally have 5 days sets and have also bo...\n",
      "2          5.0  Fabulous and worth every penny. Used for clean...\n",
      "3          5.0  A must if you love garlic on tomato marinara s...\n",
      "4          5.0  Worth every penny! Buy one now and be a pizza ...\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna(axis = 0)\n",
    "df = df[['star_rating','review_body']]\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4874562, 2)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labelling Reviews:\n",
    "## The reviews with rating 4,5 are labelled to be 1 and 1,2 are labelled as 0. Discard the reviews with rating 3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0    3124553\n",
      "4.0     731693\n",
      "1.0     426852\n",
      "3.0     349533\n",
      "2.0     241931\n",
      "Name: star_rating, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.star_rating.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   star_rating                                        review_body  label\n",
      "0          5.0                Beautiful.  Looks great on counter.      1\n",
      "1          5.0  I personally have 5 days sets and have also bo...      1\n",
      "2          5.0  Fabulous and worth every penny. Used for clean...      1 \n",
      " (3856246, 3)\n"
     ]
    }
   ],
   "source": [
    "pos_label = df[df[\"star_rating\"].isin([4,5])]\n",
    "pos_label[\"label\"]=1\n",
    "print(pos_label.head(3),\"\\n\",pos_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    star_rating                                        review_body\n",
      "9           3.0  Should have come with a kit to install drain t...\n",
      "28          3.0  Was ok....case was good for price...arrived on...\n",
      "34          3.0  Not equal to the brand, soft and thin, bust wh... \n",
      " (349533, 2)\n"
     ]
    }
   ],
   "source": [
    "neutral_label = df[df[\"star_rating\"].isin([3])]\n",
    "print(neutral_label.head(3),\"\\n\",neutral_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    star_rating                                        review_body  label\n",
      "5           1.0  The description says &#34;Suitable for all typ...      0\n",
      "24          1.0                    I hate it I cook in regular pot      0\n",
      "25          2.0  The velcro does not hold well.  Does stat cold...      0 \n",
      " (668783, 3)\n"
     ]
    }
   ],
   "source": [
    "neg_label = df[df[\"star_rating\"].isin([1,2])]\n",
    "neg_label[\"label\"]=0\n",
    "print(neg_label.head(3),\"\\n\",neg_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## We select 200000 reviews randomly with 100,000 positive and 100,000 negative reviews.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_pos_labels = pos_label.sample(n=100000)\n",
    "rnd_neg_labels = neg_label.sample(n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   star_rating                                        review_body  label\n",
      "0          5.0  The price was perfect, its and solid and there...      1\n",
      "1          5.0  Our toaster oven pan was worn out. So we purch...      1\n",
      "2          5.0  These coffee urns are a great buy. Well worth ...      1\n",
      "3          5.0  Beautiful craftsmanship. Unbeliveable detail i...      1\n",
      "4          5.0  I bought this mug for my wife for Xmas in 2012...      1\n",
      "5          5.0  great treat jar, have several in various sizes...      1\n",
      "6          5.0  This has been my go-to vegetable peeler for ne...      1\n",
      "7          5.0  great product.  Second time to purchase.  gave...      1\n",
      "8          5.0  These are great like all the reviews state. Gr...      1\n",
      "9          5.0  great product! super fancy! well made!! bought...      1 \n",
      "         star_rating                                        review_body  label\n",
      "199990          1.0  Crap.  Not even good enough for a kids Hallowe...      0\n",
      "199991          1.0  I purchased this thermometer, put it away for ...      0\n",
      "199992          1.0  I did not write this review.  It is a scam!  T...      0\n",
      "199993          1.0  We needed a toaster and received one (Black an...      0\n",
      "199994          1.0  Every loaf of bread I baked was sunken down in...      0\n",
      "199995          1.0  Initially it was a great grill for a small pat...      0\n",
      "199996          1.0  When my trusty old Braun automatic coffeemaker...      0\n",
      "199997          2.0  Unlike all the plastic models I've used, the c...      0\n",
      "199998          1.0  The metal gauge is thin. The clam shell shape ...      0\n",
      "199999          1.0  Ground the first cup, no problem.  Ground the ...      0\n"
     ]
    }
   ],
   "source": [
    "exp_data = pd.concat([rnd_pos_labels,rnd_neg_labels]).reset_index(drop=True)\n",
    "print(exp_data.head(10),\"\\n\",exp_data.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "## Convert the all reviews into the lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   count_before_clean\n",
      "0                 116\n",
      "1                 125\n",
      "2                 143\n",
      "3                 144\n",
      "4                 372 \n",
      " 64614217 \n",
      " 323.071085\n"
     ]
    }
   ],
   "source": [
    "## Stats Data Frame stores the count of characters after each pre-processing task\n",
    "stats = pd.DataFrame()\n",
    "stats['count_before_clean'] = exp_data['review_body'].str.len()\n",
    "print(stats.head(),\"\\n\",stats['count_before_clean'].sum(),\"\\n\",stats['count_before_clean'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   star_rating                                        review_body  label\n",
      "0          5.0  the price was perfect, its and solid and there...      1\n",
      "1          5.0  our toaster oven pan was worn out. so we purch...      1\n",
      "2          5.0  these coffee urns are a great buy. well worth ...      1\n",
      "3          5.0  beautiful craftsmanship. unbeliveable detail i...      1\n",
      "4          5.0  i bought this mug for my wife for xmas in 2012...      1\n"
     ]
    }
   ],
   "source": [
    "exp_data['review_body'] = exp_data['review_body'].str.lower()\n",
    "print(exp_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the HTML and URLs from the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   star_rating                                        review_body  label  \\\n",
      "0          5.0  the price was perfect, its and solid and there...      1   \n",
      "1          5.0  our toaster oven pan was worn out. so we purch...      1   \n",
      "2          5.0  these coffee urns are a great buy. well worth ...      1   \n",
      "3          5.0  beautiful craftsmanship. unbeliveable detail i...      1   \n",
      "4          5.0  i bought this mug for my wife for xmas in 2012...      1   \n",
      "\n",
      "                                     after_url_clean  \n",
      "0  the price was perfect, its and solid and there...  \n",
      "1  our toaster oven pan was worn out. so we purch...  \n",
      "2  these coffee urns are a great buy. well worth ...  \n",
      "3  beautiful craftsmanship. unbeliveable detail i...  \n",
      "4  i bought this mug for my wife for xmas in 2012...   \n",
      " 63855211 \n",
      " 319.276055\n"
     ]
    }
   ],
   "source": [
    "exp_data['after_url_clean'] = exp_data['review_body'].apply(lambda x: bs4.BeautifulSoup(x, 'lxml').get_text())\n",
    "exp_data['after_url_clean'] = exp_data['after_url_clean'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "stats['count_after_url_clean'] = exp_data['after_url_clean'].str.len()\n",
    "print(exp_data.head(),\"\\n\",stats['count_after_url_clean'].sum(),\"\\n\",stats['count_after_url_clean'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strip Whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   star_rating                                        review_body  label  \\\n",
      "0          5.0  the price was perfect, its and solid and there...      1   \n",
      "1          5.0  our toaster oven pan was worn out. so we purch...      1   \n",
      "2          5.0  these coffee urns are a great buy. well worth ...      1   \n",
      "3          5.0  beautiful craftsmanship. unbeliveable detail i...      1   \n",
      "4          5.0  i bought this mug for my wife for xmas in 2012...      1   \n",
      "\n",
      "                                     after_url_clean  \\\n",
      "0  the price was perfect, its and solid and there...   \n",
      "1  our toaster oven pan was worn out. so we purch...   \n",
      "2  these coffee urns are a great buy. well worth ...   \n",
      "3  beautiful craftsmanship. unbeliveable detail i...   \n",
      "4  i bought this mug for my wife for xmas in 2012...   \n",
      "\n",
      "                                   after_space_clean  \n",
      "0  the price was perfect, its and solid and there...  \n",
      "1  our toaster oven pan was worn out. so we purch...  \n",
      "2  these coffee urns are a great buy. well worth ...  \n",
      "3  beautiful craftsmanship. unbeliveable detail i...  \n",
      "4  i bought this mug for my wife for xmas in 2012...   \n",
      " 63854714 \n",
      " 319.27357\n"
     ]
    }
   ],
   "source": [
    "exp_data['after_space_clean'] = exp_data['after_url_clean'].str.strip()\n",
    "stats['count_after_spaces_clean'] = exp_data['after_space_clean'].str.len()\n",
    "print(exp_data.head(),\"\\n\",stats['count_after_spaces_clean'].sum(),\"\\n\",stats['count_after_spaces_clean'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform contractions on the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   star_rating                                        review_body  label  \\\n",
      "0          5.0  the price was perfect, its and solid and there...      1   \n",
      "1          5.0  our toaster oven pan was worn out. so we purch...      1   \n",
      "2          5.0  these coffee urns are a great buy. well worth ...      1   \n",
      "3          5.0  beautiful craftsmanship. unbeliveable detail i...      1   \n",
      "4          5.0  i bought this mug for my wife for xmas in 2012...      1   \n",
      "\n",
      "                                     after_url_clean  \\\n",
      "0  the price was perfect, its and solid and there...   \n",
      "1  our toaster oven pan was worn out. so we purch...   \n",
      "2  these coffee urns are a great buy. well worth ...   \n",
      "3  beautiful craftsmanship. unbeliveable detail i...   \n",
      "4  i bought this mug for my wife for xmas in 2012...   \n",
      "\n",
      "                                   after_space_clean  \\\n",
      "0  the price was perfect, its and solid and there...   \n",
      "1  our toaster oven pan was worn out. so we purch...   \n",
      "2  these coffee urns are a great buy. well worth ...   \n",
      "3  beautiful craftsmanship. unbeliveable detail i...   \n",
      "4  i bought this mug for my wife for xmas in 2012...   \n",
      "\n",
      "                               after_contraction_fix  \n",
      "0  the price was perfect, its and solid and there...  \n",
      "1  our toaster oven pan was worn out. so we purch...  \n",
      "2  these coffee urns are a great buy. well worth ...  \n",
      "3  beautiful craftsmanship. unbeliveable detail i...  \n",
      "4  i bought this mug for my wife for xmas in 2012...   \n",
      " 63794984 \n",
      " 318.97492\n"
     ]
    }
   ],
   "source": [
    "exp_data['after_contraction_fix'] = exp_data['after_space_clean'].apply(lambda x: [contractions.fix(word) for word in x.split()])\n",
    "exp_data['after_contraction_fix'] = [' '.join(map(str, l)) for l in exp_data['after_contraction_fix']]\n",
    "stats['count_after_expanding_contractions'] = exp_data['after_contraction_fix'].str.len()\n",
    "print(exp_data.head(),\"\\n\",stats['count_after_expanding_contractions'].sum(),\"\\n\",stats['count_after_expanding_contractions'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove non-alphabetical characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   star_rating                                        review_body  label  \\\n",
      "0          5.0  the price was perfect, its and solid and there...      1   \n",
      "1          5.0  our toaster oven pan was worn out. so we purch...      1   \n",
      "2          5.0  these coffee urns are a great buy. well worth ...      1   \n",
      "3          5.0  beautiful craftsmanship. unbeliveable detail i...      1   \n",
      "4          5.0  i bought this mug for my wife for xmas in 2012...      1   \n",
      "\n",
      "                                     after_url_clean  \\\n",
      "0  the price was perfect, its and solid and there...   \n",
      "1  our toaster oven pan was worn out. so we purch...   \n",
      "2  these coffee urns are a great buy. well worth ...   \n",
      "3  beautiful craftsmanship. unbeliveable detail i...   \n",
      "4  i bought this mug for my wife for xmas in 2012...   \n",
      "\n",
      "                                   after_space_clean  \\\n",
      "0  the price was perfect, its and solid and there...   \n",
      "1  our toaster oven pan was worn out. so we purch...   \n",
      "2  these coffee urns are a great buy. well worth ...   \n",
      "3  beautiful craftsmanship. unbeliveable detail i...   \n",
      "4  i bought this mug for my wife for xmas in 2012...   \n",
      "\n",
      "                               after_contraction_fix  \\\n",
      "0  the price was perfect, its and solid and there...   \n",
      "1  our toaster oven pan was worn out. so we purch...   \n",
      "2  these coffee urns are a great buy. well worth ...   \n",
      "3  beautiful craftsmanship. unbeliveable detail i...   \n",
      "4  i bought this mug for my wife for xmas in 2012...   \n",
      "\n",
      "                                after_nonalpha_clean  \n",
      "0  the price was perfect its and solid and there ...  \n",
      "1  our toaster oven pan was worn out so we purcha...  \n",
      "2  these coffee urns are a great buy well worth t...  \n",
      "3  beautiful craftsmanship unbeliveable detail in...  \n",
      "4  i bought this mug for my wife for xmas in     ...   \n",
      " 62588349 \n",
      " 312.941745\n"
     ]
    }
   ],
   "source": [
    "exp_data['after_nonalpha_clean'] = exp_data.after_contraction_fix.str.replace(r'[^a-zA-Z]\\s?',r' ',regex=True)\n",
    "stats['count_after_nonalpha_clean'] = exp_data['after_nonalpha_clean'].str.len()\n",
    "print(exp_data.head(),\"\\n\",stats['count_after_nonalpha_clean'].sum(),\"\\n\",stats['count_after_nonalpha_clean'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the extra spaces between the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   star_rating                                        review_body  label  \\\n",
      "0          5.0  the price was perfect, its and solid and there...      1   \n",
      "1          5.0  our toaster oven pan was worn out. so we purch...      1   \n",
      "2          5.0  these coffee urns are a great buy. well worth ...      1   \n",
      "3          5.0  beautiful craftsmanship. unbeliveable detail i...      1   \n",
      "4          5.0  i bought this mug for my wife for xmas in 2012...      1   \n",
      "\n",
      "                                     after_url_clean  \\\n",
      "0  the price was perfect, its and solid and there...   \n",
      "1  our toaster oven pan was worn out. so we purch...   \n",
      "2  these coffee urns are a great buy. well worth ...   \n",
      "3  beautiful craftsmanship. unbeliveable detail i...   \n",
      "4  i bought this mug for my wife for xmas in 2012...   \n",
      "\n",
      "                                   after_space_clean  \\\n",
      "0  the price was perfect, its and solid and there...   \n",
      "1  our toaster oven pan was worn out. so we purch...   \n",
      "2  these coffee urns are a great buy. well worth ...   \n",
      "3  beautiful craftsmanship. unbeliveable detail i...   \n",
      "4  i bought this mug for my wife for xmas in 2012...   \n",
      "\n",
      "                               after_contraction_fix  \\\n",
      "0  the price was perfect, its and solid and there...   \n",
      "1  our toaster oven pan was worn out. so we purch...   \n",
      "2  these coffee urns are a great buy. well worth ...   \n",
      "3  beautiful craftsmanship. unbeliveable detail i...   \n",
      "4  i bought this mug for my wife for xmas in 2012...   \n",
      "\n",
      "                                after_nonalpha_clean  \n",
      "0  the price was perfect its and solid and there ...  \n",
      "1  our toaster oven pan was worn out so we purcha...  \n",
      "2  these coffee urns are a great buy well worth t...  \n",
      "3  beautiful craftsmanship unbeliveable detail in...  \n",
      "4  i bought this mug for my wife for xmas in     ...   \n",
      " 62375872 \n",
      " 311.87936\n"
     ]
    }
   ],
   "source": [
    "exp_data['after_nonalpha_clean'] = exp_data['after_nonalpha_clean'].str.strip()\n",
    "stats['count_after_nonalpha_clean'] = exp_data['after_nonalpha_clean'].str.len()\n",
    "print(exp_data.head(),\"\\n\",stats['count_after_nonalpha_clean'].sum(),\"\\n\",stats['count_after_nonalpha_clean'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove the stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   star_rating                                        review_body  label  \\\n",
      "0          5.0  the price was perfect, its and solid and there...      1   \n",
      "1          5.0  our toaster oven pan was worn out. so we purch...      1   \n",
      "2          5.0  these coffee urns are a great buy. well worth ...      1   \n",
      "3          5.0  beautiful craftsmanship. unbeliveable detail i...      1   \n",
      "4          5.0  i bought this mug for my wife for xmas in 2012...      1   \n",
      "\n",
      "                                     after_url_clean  \\\n",
      "0  the price was perfect, its and solid and there...   \n",
      "1  our toaster oven pan was worn out. so we purch...   \n",
      "2  these coffee urns are a great buy. well worth ...   \n",
      "3  beautiful craftsmanship. unbeliveable detail i...   \n",
      "4  i bought this mug for my wife for xmas in 2012...   \n",
      "\n",
      "                                   after_space_clean  \\\n",
      "0  the price was perfect, its and solid and there...   \n",
      "1  our toaster oven pan was worn out. so we purch...   \n",
      "2  these coffee urns are a great buy. well worth ...   \n",
      "3  beautiful craftsmanship. unbeliveable detail i...   \n",
      "4  i bought this mug for my wife for xmas in 2012...   \n",
      "\n",
      "                               after_contraction_fix  \\\n",
      "0  the price was perfect, its and solid and there...   \n",
      "1  our toaster oven pan was worn out. so we purch...   \n",
      "2  these coffee urns are a great buy. well worth ...   \n",
      "3  beautiful craftsmanship. unbeliveable detail i...   \n",
      "4  i bought this mug for my wife for xmas in 2012...   \n",
      "\n",
      "                                after_nonalpha_clean  \\\n",
      "0  the price was perfect its and solid and there ...   \n",
      "1  our toaster oven pan was worn out so we purcha...   \n",
      "2  these coffee urns are a great buy well worth t...   \n",
      "3  beautiful craftsmanship unbeliveable detail in...   \n",
      "4  i bought this mug for my wife for xmas in     ...   \n",
      "\n",
      "                             after_stopwords_removal  \n",
      "0  price perfect solid really much solid piece me...  \n",
      "1  toaster oven pan worn purchased pan replace fi...  \n",
      "2  coffee urns great buy well worth money make cu...  \n",
      "3  beautiful craftsmanship unbeliveable detail ar...  \n",
      "4  bought mug wife xmas swear tells every day gre...   \n",
      " 38604043 \n",
      " 193.020215\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "exp_data['after_stopwords_removal'] = exp_data['after_nonalpha_clean'].apply(lambda x: ' '.join([x for x in x.split() if x not in stop]))\n",
    "stats['count_after_removing_stopwords'] = exp_data['after_stopwords_removal'].str.len()\n",
    "print(exp_data.head(),\"\\n\",stats['count_after_removing_stopwords'].sum(),\"\\n\",stats['count_after_removing_stopwords'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Lemmatization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   star_rating                                        review_body  label  \\\n",
      "0          5.0  the price was perfect, its and solid and there...      1   \n",
      "1          5.0  our toaster oven pan was worn out. so we purch...      1   \n",
      "2          5.0  these coffee urns are a great buy. well worth ...      1   \n",
      "3          5.0  beautiful craftsmanship. unbeliveable detail i...      1   \n",
      "4          5.0  i bought this mug for my wife for xmas in 2012...      1   \n",
      "\n",
      "                                     after_url_clean  \\\n",
      "0  the price was perfect, its and solid and there...   \n",
      "1  our toaster oven pan was worn out. so we purch...   \n",
      "2  these coffee urns are a great buy. well worth ...   \n",
      "3  beautiful craftsmanship. unbeliveable detail i...   \n",
      "4  i bought this mug for my wife for xmas in 2012...   \n",
      "\n",
      "                                   after_space_clean  \\\n",
      "0  the price was perfect, its and solid and there...   \n",
      "1  our toaster oven pan was worn out. so we purch...   \n",
      "2  these coffee urns are a great buy. well worth ...   \n",
      "3  beautiful craftsmanship. unbeliveable detail i...   \n",
      "4  i bought this mug for my wife for xmas in 2012...   \n",
      "\n",
      "                               after_contraction_fix  \\\n",
      "0  the price was perfect, its and solid and there...   \n",
      "1  our toaster oven pan was worn out. so we purch...   \n",
      "2  these coffee urns are a great buy. well worth ...   \n",
      "3  beautiful craftsmanship. unbeliveable detail i...   \n",
      "4  i bought this mug for my wife for xmas in 2012...   \n",
      "\n",
      "                                after_nonalpha_clean  \\\n",
      "0  the price was perfect its and solid and there ...   \n",
      "1  our toaster oven pan was worn out so we purcha...   \n",
      "2  these coffee urns are a great buy well worth t...   \n",
      "3  beautiful craftsmanship unbeliveable detail in...   \n",
      "4  i bought this mug for my wife for xmas in     ...   \n",
      "\n",
      "                             after_stopwords_removal  \\\n",
      "0  price perfect solid really much solid piece me...   \n",
      "1  toaster oven pan worn purchased pan replace fi...   \n",
      "2  coffee urns great buy well worth money make cu...   \n",
      "3  beautiful craftsmanship unbeliveable detail ar...   \n",
      "4  bought mug wife xmas swear tells every day gre...   \n",
      "\n",
      "                                 after_lemmatization  \n",
      "0  price perfect solid really much solid piece me...  \n",
      "1  toaster oven pan worn purchased pan replace fi...  \n",
      "2  coffee urn great buy well worth money make cup...  \n",
      "3  beautiful craftsmanship unbeliveable detail ar...  \n",
      "4  bought mug wife xmas swear tell every day grea...   \n",
      " 37987564 \n",
      " 189.93782\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "exp_data['after_lemmatization'] =  exp_data['after_stopwords_removal'].apply(lambda x: ' '.join([lemmatizer.lemmatize(w) for w in nltk.word_tokenize(x)]))\n",
    "stats['count_after_lemmatization'] = exp_data['after_lemmatization'].str.len()\n",
    "print(exp_data.head(),\"\\n\",stats['count_after_lemmatization'].sum(),\"\\n\",stats['count_after_lemmatization'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160000,) (160000,) \n",
      " (40000,) (40000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(exp_data['after_lemmatization'], exp_data['label'], test_size=0.2, random_state=30)\n",
    "print(X_train.shape,Y_train.shape,\"\\n\",X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 37185)\t0.16412963075389586\n",
      "  (0, 11707)\t0.21129962797950347\n",
      "  (0, 31537)\t0.1069937751028401\n",
      "  (0, 6901)\t0.49596453999485274\n",
      "  (0, 26365)\t0.1231476144817881\n",
      "  (0, 41777)\t0.4086859246376329\n",
      "  (0, 17102)\t0.18208612005233038\n",
      "  (0, 10054)\t0.15596926375422737\n",
      "  (0, 29264)\t0.2572252268631152\n",
      "  (0, 3121)\t0.22940357339763728\n",
      "  (0, 35473)\t0.12475602403727826\n",
      "  (0, 36276)\t0.15348870599383874\n",
      "  (0, 26970)\t0.13599310668959044\n",
      "  (0, 159)\t0.23081670639911528\n",
      "  (0, 4468)\t0.2555402405836904\n",
      "  (0, 5289)\t0.31713255034884197\n",
      "  (0, 31814)\t0.18909294298846485\n",
      "  (1, 51829)\t0.09250212934074889\n",
      "  (1, 42428)\t0.20390386576925912\n",
      "  (1, 168)\t0.32536864882549227\n",
      "  (1, 51882)\t0.23088744478475887\n",
      "  (1, 34497)\t0.2756534446598014\n",
      "  (1, 45026)\t0.35627056943154756\n",
      "  (1, 48637)\t0.3992269351206103\n",
      "  (1, 8613)\t0.17925205547070538\n",
      "  :\t:\n",
      "  (159999, 51510)\t0.11490017065546101\n",
      "  (159999, 31356)\t0.07434579855271252\n",
      "  (159999, 7351)\t0.07816338115084633\n",
      "  (159999, 41351)\t0.05908218726144503\n",
      "  (159999, 21294)\t0.06344057395828397\n",
      "  (159999, 33443)\t0.08387270679574065\n",
      "  (159999, 3296)\t0.06153912771171126\n",
      "  (159999, 26927)\t0.056141244519533375\n",
      "  (159999, 26000)\t0.06827496500214958\n",
      "  (159999, 47963)\t0.07305839344302709\n",
      "  (159999, 50386)\t0.055315497447245315\n",
      "  (159999, 30175)\t0.052778861438492634\n",
      "  (159999, 47593)\t0.05641566142273411\n",
      "  (159999, 15696)\t0.046264394428082105\n",
      "  (159999, 3227)\t0.04991412067633805\n",
      "  (159999, 5268)\t0.23588776995488467\n",
      "  (159999, 11307)\t0.10429430723120646\n",
      "  (159999, 17253)\t0.05428557390588159\n",
      "  (159999, 44225)\t0.07274182567119034\n",
      "  (159999, 46984)\t0.07079817091480732\n",
      "  (159999, 49387)\t0.05013287575478811\n",
      "  (159999, 49315)\t0.03502934942282439\n",
      "  (159999, 51829)\t0.036083663593692465\n",
      "  (159999, 19039)\t0.08091633784386537\n",
      "  (159999, 36276)\t0.04819528222655495 \n",
      "   (0, 47505)\t0.5957695174802912\n",
      "  (0, 30422)\t0.39580188638841596\n",
      "  (0, 22190)\t0.5187287271008667\n",
      "  (0, 7467)\t0.46831619281526743\n",
      "  (1, 51381)\t0.12093033501162831\n",
      "  (1, 50861)\t0.06749996732980457\n",
      "  (1, 46352)\t0.09264543529473371\n",
      "  (1, 46333)\t0.07994728032441237\n",
      "  (1, 41544)\t0.12056966257984876\n",
      "  (1, 41173)\t0.14913602774013576\n",
      "  (1, 37987)\t0.10734658110922228\n",
      "  (1, 35207)\t0.09818711677674774\n",
      "  (1, 35121)\t0.13567183658891868\n",
      "  (1, 34060)\t0.10335146617941245\n",
      "  (1, 31537)\t0.10918804822230478\n",
      "  (1, 31378)\t0.13569459942970755\n",
      "  (1, 29664)\t0.0757659663879072\n",
      "  (1, 26529)\t0.14976999298665028\n",
      "  (1, 26504)\t0.11704428697563089\n",
      "  (1, 25062)\t0.1762329176031096\n",
      "  (1, 24894)\t0.08532665627845912\n",
      "  (1, 23981)\t0.08298468166881509\n",
      "  (1, 22244)\t0.11181093174708848\n",
      "  (1, 21821)\t0.10753208443262167\n",
      "  (1, 21536)\t0.14856019255557687\n",
      "  :\t:\n",
      "  (39999, 33392)\t0.13851640346411917\n",
      "  (39999, 33378)\t0.10984160526995271\n",
      "  (39999, 32725)\t0.23152549086405938\n",
      "  (39999, 32704)\t0.18966330503296988\n",
      "  (39999, 31669)\t0.1344250480774321\n",
      "  (39999, 30332)\t0.11169755093359855\n",
      "  (39999, 29836)\t0.1423116255837303\n",
      "  (39999, 28876)\t0.13204146807318884\n",
      "  (39999, 27471)\t0.08750128046006869\n",
      "  (39999, 26825)\t0.1007691110926933\n",
      "  (39999, 19980)\t0.3343995859061545\n",
      "  (39999, 19004)\t0.2276817189801202\n",
      "  (39999, 18424)\t0.1763420796543705\n",
      "  (39999, 17850)\t0.2256718679116784\n",
      "  (39999, 17377)\t0.1608627883653343\n",
      "  (39999, 15764)\t0.26494279117284264\n",
      "  (39999, 15744)\t0.11757289784302531\n",
      "  (39999, 14549)\t0.15100623925646495\n",
      "  (39999, 14316)\t0.19359308599254302\n",
      "  (39999, 9723)\t0.12688802720619943\n",
      "  (39999, 8161)\t0.10322227699496977\n",
      "  (39999, 6152)\t0.09779289513539403\n",
      "  (39999, 6008)\t0.21160525417992862\n",
      "  (39999, 5205)\t0.09051029704309858\n",
      "  (39999, 3227)\t0.10419463793261866\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer= TfidfVectorizer()\n",
    "tf_x_train = vectorizer.fit_transform(X_train)\n",
    "tf_x_test = vectorizer.transform(X_test)\n",
    "print(tf_x_train,\"\\n\",tf_x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "perceptron = Perceptron(tol=1e-3, random_state=0)\n",
    "perceptron.fit(tf_x_train,Y_train)\n",
    "y_test_pred=perceptron.predict(tf_x_test)\n",
    "y_train_pred=perceptron.predict(tf_x_train)\n",
    "test_report=classification_report(Y_test,y_test_pred,output_dict=True)\n",
    "train_report=classification_report(Y_train,y_train_pred,output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.902575\n",
      "0.9163555084691016\n",
      "0.8861305113757043\n",
      "0.9009895959044195\n",
      "0.8567\n",
      "0.8714949610986371\n",
      "0.8361304543860528\n",
      "0.8534465125792596\n"
     ]
    }
   ],
   "source": [
    "train_metrics = [train_report['accuracy'],train_report['1']['precision'],train_report['1']['recall'],train_report['1']['f1-score']]\n",
    "print(*train_metrics,sep=\"\\n\")\n",
    "test_metrics = [test_report['accuracy'],test_report['1']['precision'],test_report['1']['recall'],test_report['1']['f1-score']]\n",
    "print(*test_metrics,sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "svm = LinearSVC(random_state=0)\n",
    "svm.fit(tf_x_train,Y_train)\n",
    "y_test_pred=svm.predict(tf_x_test)\n",
    "y_train_pred=svm.predict(tf_x_train)\n",
    "test_report=classification_report(Y_test,y_test_pred,output_dict=True)\n",
    "train_report=classification_report(Y_train,y_train_pred,output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.93375\n",
      "0.9344553588187449\n",
      "0.9330076587663514\n",
      "0.9337309476474486\n",
      "0.896075\n",
      "0.9004662477194405\n",
      "0.8901357647412454\n",
      "0.8952712065099638\n"
     ]
    }
   ],
   "source": [
    "train_metrics = [train_report['accuracy'],train_report['1']['precision'],train_report['1']['recall'],train_report['1']['f1-score']]\n",
    "print(*train_metrics,sep=\"\\n\")\n",
    "test_metrics = [test_report['accuracy'],test_report['1']['precision'],test_report['1']['recall'],test_report['1']['f1-score']]\n",
    "print(*test_metrics,sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(max_iter=1000,solver='saga')\n",
    "lr.fit(tf_x_train,Y_train)\n",
    "y_test_pred=lr.predict(tf_x_test)\n",
    "y_train_pred=lr.predict(tf_x_train)\n",
    "test_report=classification_report(Y_test,y_test_pred,output_dict=True)\n",
    "train_report=classification_report(Y_train,y_train_pred,output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.91381875\n",
      "0.916782002566748\n",
      "0.910356201351841\n",
      "0.9135578026166491\n",
      "0.899225\n",
      "0.9054673182651192\n",
      "0.8910876208606783\n",
      "0.8982199217270546\n"
     ]
    }
   ],
   "source": [
    "train_metrics = [train_report['accuracy'],train_report['1']['precision'],train_report['1']['recall'],train_report['1']['f1-score']]\n",
    "print(*train_metrics,sep=\"\\n\")\n",
    "test_metrics = [test_report['accuracy'],test_report['1']['precision'],test_report['1']['recall'],test_report['1']['f1-score']]\n",
    "print(*test_metrics,sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mul_model = MultinomialNB()\n",
    "mul_model.fit(tf_x_train,Y_train)\n",
    "y_train_pred = mul_model.predict(tf_x_train)\n",
    "y_test_pred = mul_model.predict(tf_x_test)\n",
    "test_report=classification_report(Y_test, y_test_pred,output_dict=True)\n",
    "train_report=classification_report(Y_train, y_train_pred,output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8879375\n",
      "0.8897451022226684\n",
      "0.8857432001899074\n",
      "0.8877396411174695\n",
      "0.87015\n",
      "0.8747398873268031\n",
      "0.863433695706628\n",
      "0.8690500201694231\n"
     ]
    }
   ],
   "source": [
    "train_metrics = [train_report['accuracy'],train_report['1']['precision'],train_report['1']['recall'],train_report['1']['f1-score']]\n",
    "print(*train_metrics,sep=\"\\n\")\n",
    "test_metrics = [test_report['accuracy'],test_report['1']['precision'],test_report['1']['recall'],test_report['1']['f1-score']]\n",
    "print(*test_metrics,sep=\"\\n\")\\"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
